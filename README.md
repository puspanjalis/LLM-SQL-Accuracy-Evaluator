# 🔍 LLM SQL Accuracy Evaluator

This repository provides an implementation for evaluating the **accuracy of SQL queries generated by a Large Language Model (LLM)**, using both result comparison and optional LLM-based judgment.

## 💡 Use Case

Given a natural language question:
- Generate an SQL query using an LLM.
- Compare the result of this generated query to a ground-truth (golden) SQL query.
- Compute accuracy as a percentage of correct rows/columns or through LLM judgment.

## 🛠️ Features

- SQLite-based testbed for quick execution
- Golden dataset to benchmark SQL generation
- Row-wise and schema-wise result comparison
- Optional "LLM-as-a-Judge" evaluator using OpenAI or other APIs


## 📦 Installation

```bash
git clone https://github.com/puspanjalis/llm_sql_accuracy_evaluator.git
cd llm_sql_accuracy_evaluator
pip install -r requirements.txt      
```

## 🤝 Contributions
Pull requests welcome! Please open an issue to discuss improvements or bugs. Drop a message [here](https://www.linkedin.com/in/puspanjalisarma/)
  
